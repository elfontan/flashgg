{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cf0ff21",
   "metadata": {},
   "source": [
    "## Low mass diphoton analysis\n",
    "### Parametric neural network for signal and background classification (step 4: zee validation): \n",
    "* **use the model** trained in `trainingParametricNN_nearestPoint.ipynb`\n",
    "* **save NNScore in the testing trees** \n",
    "\n",
    "**SETUP**: Notebook running in [SWAN](https://swan.web.cern.ch/swan) or locally in an anaconda environent. In the second case, you might need to install the needed packages to run the notebook, e.g.:\n",
    "```\n",
    "conda install -c conda-forge matplotlib\n",
    "conda install h5py\n",
    "conda install -c conda-forge pandas\n",
    "conda install -c conda-forge uproot\n",
    "conda install uproot-methods\n",
    "pip install tensorflow\n",
    "conda install scikit-learn\n",
    "```\n",
    "\n",
    "A dedicated package that includes a set of helpers for matplotlib to more easily produce plots with the classic CMS style is installed ([mphep](https://github.com/scikit-hep/mplhep)):\n",
    "```\n",
    "pip install mplhep\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91823df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.28/00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math, os, h5py\n",
    "import pandas as pd\n",
    "import ROOT\n",
    "import uproot\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Layer\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import mplhep as hep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05cece6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Inputs: input files and input trees, training variables, mass points #\n",
    "########################################################################\n",
    "\n",
    "approach = \"nearest_flat\"\n",
    "#approach = \"nearest_15and55_nowsig\"\n",
    "#approach = \"nearest_15and55\"\n",
    "#approach = \"random\"\n",
    "\n",
    "model_file = \"models/model_WeightedCrossEntropyLoss_nearestPoint_newSamples_flat.h5\"\n",
    "#model_file = \"models/model_WeightedCrossEntropyLoss_nearestPoint_100_epochs80_15and55_newSamples.h5\"\n",
    "#model_file = \"models/model_WeightedCrossEntropyLoss.h5\"\n",
    "\n",
    "mass_points = [25, 35, 45, 65]\n",
    "\n",
    "# List of signal files\n",
    "file_names_sig = [\n",
    "    #'NTUPLES_Apr2024/ggh/ggH_M5.root', \n",
    "    'NTUPLES_Apr2024/ggh/ggH_M25.root',\n",
    "    'NTUPLES_Apr2024/ggh/ggH_M35.root',\n",
    "    'NTUPLES_Apr2024/ggh/ggH_M45.root',\n",
    "    'NTUPLES_Apr2024/ggh/ggH_M65.root'    \n",
    "]\n",
    "\n",
    "# List of signal tree names\n",
    "tree_names_sig = [\n",
    "    #'ggh_5_13TeV_UntaggedTag_0',\n",
    "    'ggh_25_13TeV_UntaggedTag_0',\n",
    "    'ggh_35_13TeV_UntaggedTag_0',\n",
    "    'ggh_45_13TeV_UntaggedTag_0',\n",
    "    'ggh_65_13TeV_UntaggedTag_0'\n",
    "]\n",
    "\n",
    "# Using the sideband tree for the data\n",
    "file_names_bkg = [\n",
    "    'NTUPLES_Apr2024/EGamma_All_Summer20UL.root',\n",
    "    #'NTUPLES_Oct2023/bkg_dipho/diPhoton_all.root'\n",
    "    'NTUPLES_Apr2024/output_ParaDDFullNorms.root'\n",
    "]\n",
    "\n",
    "# List of background tree names\n",
    "tree_names_bkg = [\n",
    "    'Data_13TeV_UntaggedTag_0',\n",
    "    'mgg_bkg_13TeV_UntaggedTag_0'\n",
    "]\n",
    "\n",
    "# Variables used for training the model \n",
    "vars_list = [\n",
    "    'dipho_lead_ptoM', 'dipho_sublead_ptoM',\n",
    "    'dipho_leadIDMVA', 'dipho_subleadIDMVA',\n",
    "    'leadEta', 'subleadEta',\n",
    "    'sigmaMrvoM', 'sigmaMwvoM',\n",
    "    'cosphi', 'vtxprob',\n",
    "    'CMS_hgg_mass'\n",
    "]\n",
    "\n",
    "# Standardizing variables ranges: reference values used in the training\n",
    "# ---------------------------------------------------------------------\n",
    "if (approach == \"nearest\"):\n",
    "    mean_REF = np.array([1.02723945e+00,5.97155929e-01,3.27149360e-01,1.40618341e-01,1.30073259e-03,1.84589661e-03,2.26421862e-02,2.87247697e-02,-2.43791035e-01,7.45146274e-01,5.09008220e+01])\n",
    "    std_REF = np.array([0.85487014,0.444752,0.5892425,0.59489149,0.99642843,0.95968482,0.02581517,0.02511732,0.77730978,0.26923345,16.86734849])\n",
    "    #mean_REF = np.array([1.02221586e+00, 5.92401851e-01, 3.51856716e-01, 1.73320566e-01, 1.37241799e-03, 1.43889548e-03, 1.98240597e-02, 2.66782502e-02, -2.30134385e-01, 7.54484348e-01, 5.10071833e+01])\n",
    "    #std_REF = np.array([0.7890904, 0.40491352, 0.58262894, 0.59625774, 1.00148242, 0.9676065, 0.02543319, 0.02461514, 0.77499453, 0.26648426, 16.91191081])\n",
    "elif (approach == \"nearest_flat\"):\n",
    "    mean_REF = np.array([1.02723945e+00,5.97155929e-01,3.27149360e-01,1.40618341e-01,1.30073259e-03,1.84589661e-03,2.26421862e-02,2.87247697e-02,-2.43791035e-01,7.45146274e-01,5.09008220e+01])\n",
    "    std_REF = np.array([0.85487014,0.444752,0.5892425,0.59489149,0.99642843,0.95968482,0.02581517,0.02511732,0.77730978,0.26923345,16.86734849])\n",
    "elif (approach == \"random\"):\n",
    "    mean_REF =  np.array([9.92453636e-01, 5.76560285e-01, 3.64651421e-01, 1.86627699e-01, 1.89154785e-03, 1.51795157e-03, 1.95562936e-02, 2.63900476e-02, -2.68335983e-01, 7.53037223e-01, 4.09308475e+01])\n",
    "    std_REF =  np.array([0.79372193, 0.41348133, 0.57886452, 0.59570215, 1.00704125, 0.97323678, 0.02527091, 0.02446731, 0.7649603, 0.26688041, 20.21055606])\n",
    "elif (approach == \"nearest_15and55\"):\n",
    "    mean_REF = np.array([1.02483978e+00,5.93576254e-01,3.54571172e-01,1.77269594e-01,1.21949066e-03,1.34274414e-03,1.97528931e-02,2.66213316e-02,-2.28972305e-01,7.54759911e-01,5.09675932e+01])\n",
    "    std_REF = np.array([0.82153747,0.42615342,0.58191029,0.59636884,1.0013576, 0.96770523,0.02534445,0.02452893,0.77483223,0.26647642,16.76293338])\n",
    "elif (approach == \"nearest_15and55_nowsig\"):\n",
    "    mean_REF = np.array([1.02483978e+00,5.93576254e-01,3.54571172e-01,1.77269594e-01,1.21949066e-03,1.34274414e-03,1.97528931e-02,2.66213316e-02,-2.28972305e-01,7.54759911e-01,5.09675932e+01])\n",
    "    std_REF = np.array([ 0.82153747,0.42615342,0.58191029,0.59636884,1.0013576,0.96770523,0.02534445,0.02452893,0.77483223,0.26647642,16.76293338])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2335a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Parametric classifier and loss function definition #\n",
    "######################################################\n",
    "class ParametricClassifier(Model):\n",
    "    def __init__(self, input_shape, architecture=[1, 4, 1], activation='relu', name=None, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        kernel_constraint = None\n",
    "        self.hidden_layers = [Dense(architecture[i+1], input_shape=(architecture[i],), activation=activation) for i in range(len(architecture)-2)]\n",
    "        self.output_layer  = Dense(architecture[-1], input_shape=(architecture[-2],), activation='linear')\n",
    "        self.build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        for i, hidden_layer in enumerate(self.hidden_layers):\n",
    "            x = hidden_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "def QuadraticLoss(true, pred):\n",
    "    y = true[:, 1] #taken from the true_label definition \n",
    "    w = true[:, 0] #taken from the true_label definition \n",
    "    f = pred[:, 0]\n",
    "    c = 1./(1+tf.exp(f))\n",
    "    return 1000*tf.reduce_mean(y*w*c**2 + (1-y)*w*(1-c)**2)\n",
    " \n",
    "def WeightedCrossEntropyLoss(true, pred):\n",
    "    y   = true[:, 1] #taken from the true_label definition \n",
    "    w   = true[:, 0] #taken from the true_label definition\n",
    "    f   = pred[:, 0]\n",
    "    return 0.1*tf.reduce_sum((1-y)*w*tf.math.log(1+tf.exp(f)) + y*w*tf.math.log(1+tf.exp(-1*f))) #activation='linear'\n",
    "\n",
    "#####################\n",
    "# Model declaration #\n",
    "#####################\n",
    "architecture = [11, 50, 50, 50, 1] # f(x, m) = NN(x, {w}, {b}) || f(x, m) = m * NN(x, {w}, {b})\n",
    "#architecture = [11, 100, 100, 100, 1] # f(x, m) = NN(x, {w}, {b}) || f(x, m) = m * NN(x, {w}, {b})\n",
    "model = ParametricClassifier(input_shape=(None, 11), architecture=architecture, activation='relu')\n",
    "optimizer = tf.keras.optimizers.legacy.Adam()\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss=WeightedCrossEntropyLoss, # QuadraticLoss\n",
    "              optimizer=optimizer, \n",
    "              #metrics=['binary_accuracy']\n",
    "             )\n",
    "\n",
    "model.load_weights(model_file)\n",
    "\n",
    "# Open the HDF5 file in read-only mode to check the content of the model\n",
    "# ----------------------------------------------------------------------\n",
    "with h5py.File(model_file, 'r') as file:\n",
    "    # Print the keys at the root level of the HDF5 file\n",
    "    print(\"Root keys:\", list(file.keys()))\n",
    "    \n",
    "    print(\"Contents of 'dense':\", list(file['dense'])) # weights and vias for each layer\n",
    "    print(\"Contents of 'dense_1':\", list(file['dense_1']))\n",
    "    print(\"Contents of 'dense_2':\", list(file['dense_2']))\n",
    "    print(\"Contents of 'dense_3':\", list(file['dense_3']))\n",
    "    \n",
    "    # Access a specific group or dataset within the HDF5 file\n",
    "    if 'top_level_model_weights' in file:\n",
    "        print(\"Contents of 'top_level_model_weights':\", list(file['top_level_model_weights']))\n",
    "    else:\n",
    "        print(\"Group or dataset 'top_level_model_weights' not found.\")\n",
    "        \n",
    "with h5py.File(model_file, 'r') as file:\n",
    "    layer_weights = file['dense']['dense']\n",
    "    print(\"Weights of dense layer:\", layer_weights)\n",
    "    \n",
    "        \n",
    "with h5py.File(model_file, 'r') as file:\n",
    "    layer_weights = file['dense_1']['dense_1']\n",
    "    print(\"Weights of dense_1 layer:\", layer_weights)\n",
    "    \n",
    "        \n",
    "with h5py.File(model_file, 'r') as file:\n",
    "    layer_weights = file['dense_2']['dense_2']\n",
    "    print(\"Weights of dense_2 layer:\", layer_weights)\n",
    "    \n",
    "        \n",
    "with h5py.File(model_file, 'r') as file:\n",
    "    layer_weights = file['dense_3']['dense_3']\n",
    "    print(\"Weights of dense_3 layer:\", layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01217998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Building the datasets #\n",
    "#########################\n",
    "\n",
    "# ---------- #\n",
    "# BACKGROUND #   \n",
    "# ---------- #\n",
    "array_bkg_data = []\n",
    "array_bkg_dipho = []\n",
    "\n",
    "# Loop to get the values for each column for the bkg sample\n",
    "# ---------------------------------------------------------\n",
    "for f, tree_name_bkg in enumerate(tree_names_bkg):\n",
    "    file_name_bkg = file_names_bkg[f]\n",
    "    file = ROOT.TFile.Open(file_name_bkg)\n",
    "    tree = file.Get(\"tagsDumper/trees/\" + tree_name_bkg)\n",
    "    rdf = ROOT.RDataFrame(tree)\n",
    "\n",
    "    if (f==0):\n",
    "        npdf_data = rdf.AsNumpy(vars_list)\n",
    "        array_bkg_data.append(npdf_data)\n",
    "    elif (f==1):\n",
    "        npdf_dipho = rdf.AsNumpy(vars_list)\n",
    "        array_bkg_dipho.append(npdf_dipho)\n",
    "        \n",
    "bkg_data_col = [np.hstack([b[var] for b in array_bkg_data]) for var in vars_list]\n",
    "bkg_data = np.column_stack(bkg_data_col)\n",
    "\n",
    "bkg_dipho_col = [np.hstack([bmc[var] for bmc in array_bkg_dipho]) for var in vars_list]\n",
    "bkg_dipho = np.column_stack(bkg_dipho_col)\n",
    "\n",
    "# ---------- #\n",
    "#   SIGNAL   #\n",
    "# ---------- #\n",
    "#array_sig5 = []\n",
    "#array_sig15 = []\n",
    "array_sig25 = []\n",
    "array_sig35 = []\n",
    "array_sig45 = []\n",
    "#array_sig55 = []\n",
    "array_sig65 = []\n",
    "\n",
    "# Loop to get the values for each column for signal samples\n",
    "# ---------------------------------------------------------\n",
    "for f, tree_name_sig in enumerate(tree_names_sig):\n",
    "    file_name_sig = file_names_sig[f]\n",
    "    file = ROOT.TFile.Open(file_name_sig)\n",
    "    tree = file.Get(\"tagsDumper/trees/\" + tree_name_sig)\n",
    "    #tree.Print()\n",
    "    rdf = ROOT.RDataFrame(tree)\n",
    "    #npdf = rdf.AsNumpy(vars_list)\n",
    "    \n",
    "    #if (f==0):\n",
    "    #    npdf5 = rdf.AsNumpy(vars_list)\n",
    "    #    array_sig5.append(npdf5)\n",
    "    #elif (f==1):\n",
    "    #    npdf15 = rdf.AsNumpy(vars_list)\n",
    "    #    array_sig15.append(npdf15)\n",
    "    if (f==0):\n",
    "        npdf25 = rdf.AsNumpy(vars_list)\n",
    "        array_sig25.append(npdf25)\n",
    "    elif (f==1):\n",
    "        npdf35 = rdf.AsNumpy(vars_list)\n",
    "        array_sig35.append(npdf35)\n",
    "    elif (f==2):\n",
    "        npdf45 = rdf.AsNumpy(vars_list)\n",
    "        array_sig45.append(npdf45)\n",
    "    #elif (f==5):\n",
    "    #    npdf55 = rdf.AsNumpy(vars_list)\n",
    "    #    array_sig55.append(npdf55)\n",
    "    elif (f==3):\n",
    "        npdf65 = rdf.AsNumpy(vars_list)\n",
    "        array_sig65.append(npdf65)\n",
    "\n",
    "#sig5_col = [np.hstack([s[var] for s in array_sig5]) for var in vars_list]\n",
    "#sig5 = np.column_stack(sig5_col)\n",
    "\n",
    "#sig15_col = [np.hstack([s[var] for s in array_sig15]) for var in vars_list]\n",
    "#sig15 = np.column_stack(sig15_col)\n",
    "\n",
    "sig25_col = [np.hstack([s[var] for s in array_sig25]) for var in vars_list]\n",
    "sig25 = np.column_stack(sig25_col)\n",
    "\n",
    "sig35_col = [np.hstack([s[var] for s in array_sig35]) for var in vars_list]\n",
    "sig35 = np.column_stack(sig35_col)\n",
    "\n",
    "sig45_col = [np.hstack([s[var] for s in array_sig45]) for var in vars_list]\n",
    "sig45 = np.column_stack(sig45_col)\n",
    "\n",
    "#sig55_col = [np.hstack([s[var] for s in array_sig55]) for var in vars_list]\n",
    "#sig55 = np.column_stack(sig55_col)\n",
    "\n",
    "sig65_col = [np.hstack([s[var] for s in array_sig65]) for var in vars_list]\n",
    "sig65 = np.column_stack(sig65_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacbcf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# --STANDARDIZING VARIABLES RANGES #\n",
    "####################################\n",
    "# Make the dataset invariant for scale effects.\n",
    "# Rule to transform data to be in a proper shape to apply the model\n",
    "\n",
    "def standardize_dataset(data, mean, std):\n",
    "    return (data - mean) / std\n",
    "\n",
    "#sig5 = standardize_dataset(sig5, mean_REF, std_REF)\n",
    "#sig15 = standardize_dataset(sig15, mean_REF, std_REF)\n",
    "sig25 = standardize_dataset(sig25, mean_REF, std_REF)\n",
    "sig35 = standardize_dataset(sig35, mean_REF, std_REF)\n",
    "sig45 = standardize_dataset(sig45, mean_REF, std_REF)\n",
    "#sig55 = standardize_dataset(sig55, mean_REF, std_REF)\n",
    "sig65 = standardize_dataset(sig65, mean_REF, std_REF)\n",
    "bkg_data = standardize_dataset(bkg_data, mean_REF, std_REF)\n",
    "bkg_dipho = standardize_dataset(bkg_dipho, mean_REF, std_REF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34518d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with NN scores for the MC signal events \n",
    "preds_dict = {mass: tf.sigmoid(model.predict(sig[:, :])).numpy()[:, 0] for mass, sig in zip(mass_points, [sig25, sig35, sig45, sig65])}\n",
    "\n",
    "# Make predictions for background data\n",
    "predsBkg_data = tf.sigmoid(model.predict(bkg_data[:, :])).numpy()[:, 0]\n",
    "# Make predictions for background mc (diphoton)\n",
    "predsBkg_dipho = tf.sigmoid(model.predict(bkg_dipho[:, :])).numpy()[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4240d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#1f78b4', '#36a2ae', '#6ec186', '#b2df8a', '#33a02c', '#fb9a99', '#e31a1c']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Loop through each prediction array and plot it with a different color\n",
    "for mass, color in zip(mass_points, colors):\n",
    "    preds = preds_dict[mass]\n",
    "    ax.hist(preds, bins=np.linspace(0., 1., 100), color=color, label=f'{mass} GeV', histtype='step', linewidth=2)\n",
    "\n",
    "#ax.set_yscale('log')\n",
    "#ax.set_ylim(1, 1000000)  # Set y-axis limits\n",
    "ax.set_xlabel('NN score')\n",
    "ax.set_ylabel('Events')\n",
    "ax.legend(loc='upper left')\n",
    "#plt.yscale('log')\n",
    "#plt.ylim(1, 25000)  # Set y-axis limits\n",
    "#plt.xlabel(\"MVA output\")\n",
    "#plt.ylabel(\"Events\")\n",
    "\n",
    "plt.style.use(hep.style.ROOT)\n",
    "hep.cms.label(\"Preliminary\", data=False)\n",
    "\n",
    "#plt.legend()\n",
    "#plt.show()\n",
    "plt.savefig(\"Plots/\"+approach+\"/MVAOutputs/allMVAOutputsSig.png\")\n",
    "plt.savefig(\"Plots/\"+approach+\"/MVAOutputs/allMVAOutputsSig.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7d72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "ax.hist(predsBkg_data, bins=np.linspace(0., 1., 100), color=\"turquoise\", label='All 2018 data', histtype='step', linewidth=2)\n",
    "\n",
    "# Loop through each prediction array and plot it with a different color\n",
    "for mass, color in zip(mass_points, colors):\n",
    "    preds = preds_dict[mass]\n",
    "    ax.hist(preds, bins=np.linspace(0., 1., 100), color=color, label=f'{mass} GeV', histtype='step', linewidth=2)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(1, 100000000)  # Set y-axis limits\n",
    "ax.set_xlabel('NN score')\n",
    "ax.set_ylabel('Events')\n",
    "ax.legend(loc='upper right')\n",
    "#plt.yscale('log')\n",
    "plt.ylim(1, 300000000000)  # Set y-axis limits\n",
    "#plt.xlabel(\"MVA output\")\n",
    "#plt.ylabel(\"Events\")\n",
    "\n",
    "plt.style.use(hep.style.ROOT)\n",
    "hep.cms.label(\"Preliminary\", data=False)\n",
    "\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.savefig(\"Plots/\"+approach+\"/MVAOutputs/allMVAOutputsSig_plusBkg_allData.png\")\n",
    "plt.savefig(\"Plots/\"+approach+\"/MVAOutputs/allMVAOutputsSig_plusBkg_allData.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eba654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# Add prediction to ROOT trees: signal samples  #\n",
    "#################################################\n",
    "\n",
    "import ROOT\n",
    "from array import array\n",
    "\n",
    "ROOT.gROOT.SetBatch()\n",
    "ROOT.gStyle.SetOptStat(0)\n",
    "ROOT.gStyle.SetOptTitle(0)\n",
    "\n",
    "# Loop over each mass point\n",
    "for mass_point in mass_points:\n",
    "    print(f\"Processing mass point {mass_point} GeV\")\n",
    "\n",
    "    # Open the input file for the current mass point\n",
    "    infile_name = f\"NTUPLES_Apr2024/ggh/ggH_M{mass_point}.root\"\n",
    "    #infile_name = f\"NTUPLES_Oct2023/ggh/ggH_M{mass_point}_merged.root\"\n",
    "    infile = ROOT.TFile(infile_name, \"read\")\n",
    "\n",
    "    # Create a new output file for the current mass point\n",
    "    outfile_name = \"NTUPLES_Apr2024/newFiles/\"+approach+\"/out_ggH_M\"+str(mass_point)+\"_newSamplesFlat.root\"\n",
    "    newfile_data = ROOT.TFile(outfile_name, \"recreate\")\n",
    "\n",
    "    # Create the necessary directories\n",
    "    tagsDumper = newfile_data.mkdir(\"tagsDumper\")\n",
    "    tagsDumper.cd()\n",
    "    trees = tagsDumper.mkdir(\"trees\")\n",
    "\n",
    "    # Get the old tree from the input file for the corresponding mass point\n",
    "    oldtree = infile.Get(f\"tagsDumper/trees/ggh_{mass_point}_13TeV_UntaggedTag_0\")\n",
    "    nentries = oldtree.GetEntries()\n",
    "    print(\"nentries = \", nentries)\n",
    "\n",
    "    # Create a new tree with the same structure\n",
    "    NNScore = array('f', [0.])\n",
    "    newtree = oldtree.CloneTree(0)\n",
    "    newtree.Branch(\"NNScore\", NNScore, \"NNScore/F\")\n",
    "\n",
    "    # Loop over the entries and fill the new tree with random values\n",
    "    count = 0\n",
    "    print(\"---------- Opening the tree\")\n",
    "    for i in oldtree:\n",
    "        NNScore[0] = preds_dict[mass_point][count]\n",
    "        newtree.Fill()\n",
    "        count += 1\n",
    "\n",
    "    print(\"---------- Save and close the tree\")\n",
    "    # Save the new tree to the new file\n",
    "    trees.cd()\n",
    "    newtree.Write()\n",
    "\n",
    "    # Close the current input file\n",
    "    infile.Close()\n",
    "    # Close the output file for the current mass point\n",
    "    newfile_data.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421e6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Add prediction to ROOT trees: DY MC #\n",
    "#######################################\n",
    "\n",
    "# Open the input file\n",
    "infile = ROOT.TFile(\"NTUPLES_Apr2024/Jun30_dyJets2018_zeeVal.root\", \"read\")\n",
    "\n",
    "# Create a new output files\n",
    "newfile_dy = ROOT.TFile(\"NTUPLES_Apr2024/newFiles/\"+approach+\"/out_dyJets2018_zeeVal.root\", \"recreate\")\n",
    "\n",
    "# Create the necessary directories\n",
    "tagsDumper = newfile_dy.mkdir(\"tagsDumper\")\n",
    "tagsDumper.cd()\n",
    "trees = tagsDumper.mkdir(\"trees\")\n",
    "\n",
    "# Get the old tree from the input file\n",
    "oldtree = infile.Get(\"tagsDumper/trees/DYToLL_13TeV_UntaggedTag_0\")\n",
    "nentries = oldtree.GetEntries()\n",
    "print(\"nentries = \", nentries)\n",
    "\n",
    "# Create a new tree with the same structure\n",
    "NNScore = array('f', [0.])\n",
    "newtree = oldtree.CloneTree(0)\n",
    "newtree.Branch(\"NNScore\", NNScore, \"NNScore/F\")\n",
    "    \n",
    "# Loop over the entries and fill the new tree with random values\n",
    "print(\"---------- Opening the DYToLL MC tree\")\n",
    "count = 0\n",
    "for i in oldtree:\n",
    "    #if (count == 1000): break\n",
    "    NNScore[0] = predsBkg_dy[count]\n",
    "    newtree.Fill()\n",
    "    count += 1\n",
    "    \n",
    "print(\"---------- Save and close the DYToLL MC tree\")\n",
    "# Save the new tree to the new file\n",
    "trees.cd()\n",
    "newtree.Write()\n",
    "newfile_dy.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952dc211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Add prediction to ROOT trees: background  #\n",
    "#############################################\n",
    "\n",
    "# Open the input file: all 2018 Zee data events\n",
    "infile = ROOT.TFile(\"NTUPLES_Apr2024/Jun30_data2018ABCD_zeeVal.root\", \"read\")\n",
    "\n",
    "# Create a new output file\n",
    "newfile_data = ROOT.TFile(\"NTUPLES_Apr2024/newFiles/\"+approach+\"/out_data2018ABCD_zeeVal.root\", \"recreate\")\n",
    "\n",
    "# Create the necessary directories\n",
    "tagsDumper = newfile_data.mkdir(\"tagsDumper\")\n",
    "tagsDumper.cd()\n",
    "trees = tagsDumper.mkdir(\"trees\")\n",
    "\n",
    "# Get the old tree from the input file\n",
    "oldtree = infile.Get(\"tagsDumper/trees/Data_13TeV_UntaggedTag_0\")\n",
    "nentries = oldtree.GetEntries()\n",
    "print(\"nentries = \", nentries)\n",
    "\n",
    "# Create a new tree with the same structure\n",
    "NNScore = array('f', [0.])\n",
    "newtree = oldtree.CloneTree(0)\n",
    "newtree.Branch(\"NNScore\", NNScore, \"NNScore/F\")\n",
    "    \n",
    "# Loop over the entries and fill the new tree with random values\n",
    "print(\"---------- Opening the full data tree\")\n",
    "count = 0\n",
    "for i in oldtree:\n",
    "    #if (count == 1000): break\n",
    "    if count % 500000 == 0:\n",
    "        print(f\"Processed {count} events\")\n",
    "    NNScore[0] = predsBkg_data[count]\n",
    "    newtree.Fill()\n",
    "    count += 1\n",
    "    \n",
    "print(\"---------- Save and close the full data tree\")\n",
    "# Save the new tree to the new file\n",
    "trees.cd()\n",
    "newtree.Write()\n",
    "newfile_data.Close()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
